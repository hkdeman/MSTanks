{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from server_comms import ServerComms, ServerMessageTypes\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import utils\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_message_types = ServerMessageTypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApiRunner:\n",
    "    def __init__(self):\n",
    "        self.is_connected = True\n",
    "        self.server = ServerComms(\"127.0.0.1\", 8052)\n",
    "            \n",
    "    def send(self, type, payload=None):\n",
    "        self.server.send_message(type, payload)\n",
    "        \n",
    "    def disconnect(self):\n",
    "        self.is_connected = False\n",
    "        self.server.server_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_runner = ApiRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TankRunner:\n",
    "    def __init__(self, name, api_runner):\n",
    "        self.name = name\n",
    "        self.api_runner = api_runner\n",
    "        self.start()\n",
    "        self.setup_reward_variables()\n",
    "        \n",
    "    def setup_reward_variables(self):\n",
    "        self.prev_closest_enemy_distance = None\n",
    "        self.prev_health = None\n",
    "        self.prev_num_of_enemies = None\n",
    "        self.prev_heading_difference = None\n",
    "        self.prev_fired = False\n",
    "        self.prev_closest_item_distance = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.has_ended = False\n",
    "        self.state = dict(me={}, enemies={}, items={})\n",
    "        self.spawn()\n",
    "        self.receive_thread = threading.Thread(target=self.read)\n",
    "        self.receive_thread.daemon = True\n",
    "        self.receive_thread.start()\n",
    "        \n",
    "    def spawn(self):\n",
    "        self.send(server_message_types.CREATETANK, dict(Name=self.name))\n",
    "        \n",
    "    def despawn(self):\n",
    "        self.send(server_message_types.DESPAWNTANK)\n",
    "        \n",
    "    def read(self):\n",
    "        while not self.has_ended:\n",
    "            message = self.api_runner.server.read_message()\n",
    "            self.update_state(message)\n",
    "     \n",
    "    def update_state(self, message):\n",
    "        if message == None: return\n",
    "        if message['type'] == server_message_types.OBJECTUPDATE:\n",
    "            data = message['data']\n",
    "            if data == None: return\n",
    "            if 'Type' not in data.keys(): return\n",
    "            \n",
    "            if data['Type'] == 'Tank':\n",
    "                if data['Name'] == self.name:\n",
    "                    self.update_me(data)\n",
    "                elif data['Name'] != self.name:\n",
    "                    self.update_enemy(data)\n",
    "            elif data['Type'] == 'AmmoPickup' or data['Type'] == 'HealthPickup':\n",
    "                self.update_item(data)\n",
    "    \n",
    "    def update_me(self, data):\n",
    "        self.state['me'] = data\n",
    "    \n",
    "    def update_enemy(self, data):\n",
    "        self.state['enemies'][data['Name']] = data\n",
    "    \n",
    "    def update_item(self, data):\n",
    "        self.state['items'][data['Id']] = data\n",
    "    \n",
    "    def send(self, type, payload=None):\n",
    "        self.api_runner.send(type, payload)\n",
    "    \n",
    "    def stop(self):\n",
    "        self.has_ended = True\n",
    "        self.despawn()\n",
    "        \n",
    "    def do_action(self, action_type):\n",
    "        if action_type == 0:\n",
    "            self.send(server_message_types.TOGGLEFORWARD)\n",
    "        elif action_type == 1:\n",
    "            self.send(server_message_types.TOGGLEREVERSE)\n",
    "        elif action_type == 2:\n",
    "            self.send(server_message_types.TOGGLELEFT)\n",
    "        elif action_type == 3:\n",
    "            self.send(server_message_types.TOGGLERIGHT)\n",
    "        elif action_type == 4:\n",
    "            self.send(server_message_types.TOGGLETURRETLEFT)\n",
    "        elif action_type == 5:\n",
    "            self.send(server_message_types.TOGGLETURRETRIGHT)\n",
    "        elif action_type == 6:\n",
    "            self.prev_fired = True\n",
    "            self.send(server_message_types.FIRE)\n",
    "        elif action_type == 7:\n",
    "            self.send(server_message_types.STOPMOVE)\n",
    "        elif action_type == 8:\n",
    "            self.send(server_message_types.STOPTURN)\n",
    "        elif action_type == 9:\n",
    "            self.send(server_message_types.STOPTURRET)\n",
    "        elif action_type == 10:\n",
    "            self.send(server_message_types.STOPALL)\n",
    "    \n",
    "    def get_closest_goal(self):\n",
    "        me_coords = (self.state['me']['X'], self.state['me']['Y'])\n",
    "        left_goal_coords = (0, 100)\n",
    "        right_goal_coords = (0, -100)\n",
    "        left_goal_distance = utils.calculate_distance(me_coords, left_goal_coords)\n",
    "        right_goal_distance = utils.calculate_distance(me_coords, right_goal_coords)\n",
    "        if (left_goal_distance < right_goal_distance):\n",
    "            return left_goal_coords\n",
    "        return right_goal_coords\n",
    "    \n",
    "    def get_closest_enemy(self):\n",
    "        min_coords = (-99999, -99999)\n",
    "        min_distance = -99999\n",
    "        if len(self.state['enemies']) == 0: \n",
    "            return min_coords, self.prev_closest_enemy_distance if self.prev_closest_enemy_distance != None else abs(min_distance)\n",
    "        \n",
    "        me_coords = (self.state['me']['X'], self.state['me']['Y'])\n",
    "        for enemy in self.state['enemies'].values():\n",
    "            enemy_coords = (enemy['X'], enemy['Y'])\n",
    "            distance = utils.calculate_distance(me_coords, enemy_coords)\n",
    "            if min_distance == -99999 or min_distance < distance:\n",
    "                min_distance = distance\n",
    "                min_coords = enemy_coords\n",
    "        return min_coords, round(min_distance, 3)\n",
    "    \n",
    "    def get_closest_item(self):\n",
    "        min_coords = (-99999, -99999)\n",
    "        min_distance = -99999\n",
    "        if len(self.state['items']) == 0: \n",
    "            return min_coords, self.prev_closest_item_distance if self.prev_closest_item_distance != None else abs(min_distance)\n",
    "        \n",
    "        me_coords = (self.state['me']['X'], self.state['me']['Y'])\n",
    "        for item in self.state['items'].values():\n",
    "            item_coords = (item['X'], item['Y'])\n",
    "            distance = utils.calculate_distance(me_coords, item_coords)\n",
    "            if min_distance == -99999 or min_distance < distance:\n",
    "                min_distance = distance\n",
    "                min_coords = item_coords\n",
    "        return min_coords, round(min_distance, 3)\n",
    "    \n",
    "    def get_number_of_enemies(self):\n",
    "        return len(self.state['enemies'].keys())\n",
    "    \n",
    "    def calculate_reward(self):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if len(self.state['me'].keys()) == 0: \n",
    "            return reward, done\n",
    "        \n",
    "        # reward on previous distance is less than normal from the closest enemy\n",
    "        if self.prev_closest_enemy_distance == None:\n",
    "            _, self.prev_closest_enemy_distance = self.get_closest_enemy()\n",
    "        else:\n",
    "            _, current_closest_enemy_distance = self.get_closest_enemy()\n",
    "            reward += 5 if self.prev_closest_enemy_distance > current_closest_enemy_distance else 0\n",
    "            reward += -5 if self.prev_closest_enemy_distance < current_closest_enemy_distance else 0\n",
    "            self.prev_closest_enemy_distance = current_closest_enemy_distance\n",
    "            \n",
    "        # reward on previous distance is less than normal from the closest enemy\n",
    "        if self.prev_closest_item_distance == None:\n",
    "            _, self.prev_closest_item_distance = self.get_closest_item()\n",
    "        else:\n",
    "            _, current_closest_item_distance = self.get_closest_item()\n",
    "            reward += 5 if self.prev_closest_item_distance > current_closest_item_distance else 0\n",
    "            reward += -5 if self.prev_closest_item_distance < current_closest_item_distance else 0\n",
    "            self.prev_closest_item_distance = current_closest_item_distance\n",
    "        \n",
    "        # reward on health increase and vice versa\n",
    "        if self.prev_health == None:\n",
    "            self.prev_health = self.state['me']['Health']\n",
    "        else:\n",
    "            current_health = self.state['me']['Health']\n",
    "            reward += 25 if self.prev_health > current_health else 0\n",
    "            reward += -10 if self.prev_health < current_health else 0\n",
    "            self.prev_health = current_health\n",
    "        \n",
    "        # reward if enemies are smaller\n",
    "        if self.prev_num_of_enemies == None:\n",
    "            self.prev_num_of_enemies = len(self.state['enemies'].keys())\n",
    "        else:\n",
    "            current_num_of_enemies = len(self.state['enemies'].keys())\n",
    "            reward += 50 if self.prev_num_of_enemies < current_num_of_enemies else 0\n",
    "            self.prev_num_of_enemies = current_num_of_enemies\n",
    "            \n",
    "        # reward on heading difference is less than normal heading difference\n",
    "        if self.prev_heading_difference == None:\n",
    "            me_coords = (self.state['me']['X'], self.state['me']['Y'])\n",
    "            enemy_coords, _ = self.get_closest_enemy()\n",
    "            self.prev_heading_difference = utils.calculate_heading(me_coords, enemy_coords)\n",
    "        else:\n",
    "            me_coords = (self.state['me']['X'], self.state['me']['Y'])\n",
    "            enemy_coords, _ = self.get_closest_enemy()\n",
    "            current_heading_difference = utils.calculate_heading(me_coords, enemy_coords)\n",
    "            reward += 10 if abs(self.prev_heading_difference - current_heading_difference) < 5 else 0\n",
    "            reward += 30 if abs(self.prev_heading_difference - current_heading_difference) < 5 and self.prev_fired else 0            \n",
    "            self.prev_heading_difference = current_heading_difference\n",
    "            \n",
    "        # reward on fire\n",
    "        if self.prev_fired:\n",
    "            reward -= 5\n",
    "            self.prev_fired = False\n",
    "        \n",
    "        # reward (high) if heading is close to the turret heading\n",
    "        if abs(self.state['me']['Heading'] - self.state['me']['TurretHeading']) < 10:\n",
    "            reward += 10\n",
    "        \n",
    "        # high rewards for going closer to goal if kills > 0\n",
    "        # TODO\n",
    "        \n",
    "        # if dead once\n",
    "        if self.state['me']['Health'] == 0:\n",
    "            done = True\n",
    "        \n",
    "        return reward, done\n",
    "    \n",
    "    def get_state_details(self):\n",
    "        if len(self.state['me'].keys()) == 0: \n",
    "            return np.array([-9999]*12)\n",
    "        \n",
    "        me_coords = [round(self.state['me']['X'],3), round(self.state['me']['Y'],3)]\n",
    "        closest_enemy_coords, closest_enemy_distance = self.get_closest_enemy()\n",
    "        number_of_enemies = self.get_number_of_enemies()\n",
    "        nearest_goal = self.get_closest_goal()\n",
    "        heading = self.state['me']['Heading']\n",
    "        turret_heading = self.state['me']['TurretHeading']\n",
    "        health = self.state['me']['Health']\n",
    "        closest_item_coords, closest_item_distance = self.get_closest_item()\n",
    "        # kills\n",
    "        # TODO\n",
    "        return np.array([\n",
    "                         round(me_coords[0], 3),\n",
    "                         round(me_coords[1], 3),\n",
    "                         round(heading, 3),\n",
    "                         round(turret_heading, 3),\n",
    "                         health,\n",
    "                         round(closest_enemy_coords[0], 3),\n",
    "                         round(closest_enemy_coords[1], 3),\n",
    "                         round(closest_item_coords[0], 3),\n",
    "                         round(closest_item_coords[1], 3),\n",
    "                         round(number_of_enemies, 3),\n",
    "                         round(nearest_goal[0], 3),\n",
    "                         round(nearest_goal[1], 3),\n",
    "                        ])\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.do_action(action)\n",
    "        time.sleep(0.2)\n",
    "        state = self.get_state_details()\n",
    "        reward, done = self.calculate_reward()\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_multiplier = 0.005\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.01\n",
    "        self.model = self._build_model()\n",
    "        self.prev_rewards = deque(maxlen=10)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randint(0, self.action_size-1)\n",
    "        \n",
    "        action = self.model.predict(state)\n",
    "        return np.argmax(action[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        summed_rewards = 0\n",
    "        \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_current = self.model.predict(state)\n",
    "            target_current[0][action] = target\n",
    "            \n",
    "            self.model.fit(state, target_current, epochs=1, verbose=0)\n",
    "            summed_rewards+=reward\n",
    "        \n",
    "        list_prev_rewards = list(self.prev_rewards)\n",
    "        if len(list_prev_rewards) == self.prev_rewards.maxlen:\n",
    "            # new accuracy is less than old one\n",
    "            if sum(list_prev_rewards[0:5]) > sum(list_prev_rewards[5:]) and self.epsilon < 1:\n",
    "                self.epsilon *= 1 + self.epsilon_multiplier\n",
    "            elif self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= 1 - self.epsilon_multiplier\n",
    "        else:\n",
    "            self.epsilon*= 1 - self.epsilon_multiplier\n",
    "            \n",
    "        self.prev_rewards.append(summed_rewards)\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 12\n",
    "action_size = 11\n",
    "n_episodes = 20\n",
    "frames = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    done = False\n",
    "    for episode in range(n_episodes):\n",
    "        tank_runner = TankRunner(\"Robo-{}\".format(episode), api_runner)\n",
    "        state = tank_runner.get_state_details()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        for frame in range(frames):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = tank_runner.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"Episode: {}/{}, score: {}, e: {:.2}\".format(episode, n_episodes, frame, agent.epsilon))\n",
    "                break\n",
    "\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "\n",
    "    #         if episode % 5 == 0:\n",
    "except Exception as e:\n",
    "    tank_runner.despawn()\n",
    "    agent.save(\"weights_episode{}.hdf5\".format(episode))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
